{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "BiLSTM_notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/exoushia/Question_question_relatedness/blob/master/BiLSTM_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS0maex8eozg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------READING LIB --------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
        "from sklearn.metrics import classification_report\n",
        "import argparse\n",
        "\n",
        "import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import KeyedVectors\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvyunFFfeozl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------------------config class --------------------------------\n",
        "class Config(object):\n",
        "\tembed_size = 300\n",
        "\thidden_layers = 1\n",
        "\thidden_size = 128\n",
        "\tbidirectional = True\n",
        "\toutput_size = 4\n",
        "\tepochs = 25\n",
        "\tlr = 0.001\n",
        "\tll_hidden_size = 50  # Linear layer hidden sizes\n",
        "    batch_size = 32\n",
        "\t# max_sen_len = 20 # Sequence length for RNN\n",
        "\tdropout_keep = 0.2\n",
        "\tsample = 1\n",
        "\tsplit_ratio = 0.4\n",
        "\tloss_fn = nn.CrossEntropyLoss()\n",
        "\tpatience = 25\n",
        "\tdelta = 0.001\n",
        "\tbatch_size_test = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0oWs4ieozp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------UTILITIES----------------------------:\n",
        "class plot_results:\n",
        "\t# Instantiation of class\n",
        "\tdef __init__(self, train_losses_plot, val_losses_plot, val_accuracies_plot,\n",
        "\t\t\t\t figname=[\"Training.png\", \"Validation.png\"], smooth=False):\n",
        "\t\tself.train_losses_plot = train_losses_plot\n",
        "\t\tself.val_losses_plot = val_losses_plot\n",
        "\t\tself.val_accuracies_plot = val_accuracies_plot\n",
        "\t\tself.smooth = smooth\n",
        "\t\tself.figname = figname\n",
        "\n",
        "\t\tflatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\t\tif smooth:\n",
        "\t\t\t# If smooth is True, taking the average of every epoch\n",
        "\t\t\tself.train_losses_plot = np.array([np.mean(loss_iter) for loss_iter in list(self.train_losses_plot)])\n",
        "\t\t\tself.val_losses_plot = np.array([np.mean(loss_iter) for loss_iter in list(self.val_losses_plot)])\n",
        "\t\t\tself.val_accuracies_plot = np.array([np.mean(loss_iter) for loss_iter in list(self.val_accuracies_plot)])\n",
        "\t\telse:\n",
        "\t\t\t# If smooth is False, flattening out the list to display avg values for every 10 iterations\n",
        "\t\t\tself.train_losses_plot = flatten(self.train_losses_plot)\n",
        "\t\t\tself.val_losses_plot = flatten(self.val_losses_plot)\n",
        "\t\t\tself.val_accuracies_plot = flatten(self.val_accuracies_plot)\n",
        "\n",
        "\t# After instantiating the class, this is the function that needs to be called:\n",
        "\tdef run(self, figure_sep=True):\n",
        "\t\t#        %matplotlib inline\n",
        "\t\tplt.style.use('classic')\n",
        "\t\tfig = plt.figure(figsize=(10, 8))\n",
        "\t\tprint(\"Starting to plot figures.... \\n\\n\")\n",
        "\t\tif figure_sep:\n",
        "\t\t\tax = plt.subplot(2, 1, 1)\n",
        "\t\t\tax.plot(self.train_losses_plot)\n",
        "\t\t\tif self.smooth:\n",
        "\t\t\t\ttitle_str = 'Average training loss for every Epoch'\n",
        "\t\t\telse:\n",
        "\t\t\t\ttitle_str = \"Training losses for every 10 iterations\"\n",
        "\t\t\tax.set(ylabel='Training set values', title=title_str)\n",
        "\t\t\tax.grid()\n",
        "\t\t\tif self.figname is not None:\n",
        "\t\t\t\tfig.savefig(self.figname[0])\n",
        "\t\t\tplt.show()\n",
        "\n",
        "\t\t\tax = plt.subplot(2, 1, 2)\n",
        "\t\t\tax.plot(self.val_losses_plot, color='blue')\n",
        "\t\t\tax.plot(self.val_accuracies_plot, color='green')\n",
        "\t\t\tax.legend(['Val losses', 'Val Accuracies'], loc='best')\n",
        "\t\t\tif self.smooth:\n",
        "\t\t\t\ttitle_str = 'Average Validation losses and accuracy for every Epoch'\n",
        "\t\t\telse:\n",
        "\t\t\t\ttitle_str = \"Validation losses and accuracy for every 10 iterations\"\n",
        "\t\t\tax.set(ylabel='Validation set values', title=title_str)\n",
        "\t\t\tax.grid()\n",
        "\t\t\tif self.figname is not None:\n",
        "\t\t\t\tfig.savefig(self.figname[1])\n",
        "\t\t\tplt.show()\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tax = plt.subplot(2, 1, 2)\n",
        "\t\t\tax.plot(self.train_losses_plot, color='red')\n",
        "\t\t\tax.plot(self.val_losses_plot, color='blue')\n",
        "\t\t\tax.plot(self.val_accuracies_plot, color='green')\n",
        "\t\t\tax.legend(['Training_loss', 'Val loss', 'Val Accuracies'], loc='best')\n",
        "\t\t\tif self.smooth:\n",
        "\t\t\t\ttitle_str = 'Average Training loss Validation losses and accuracy for every Epoch'\n",
        "\t\t\telse:\n",
        "\t\t\t\ttitle_str = \"Training loss Validation loss and accuracy for every 10 iterations\"\n",
        "\t\t\tax.set(title=title_str)\n",
        "\t\t\tax.grid()\n",
        "\t\t\tif self.figname is not None:\n",
        "\t\t\t\tfig.savefig(self.figname[0])\n",
        "\t\t\tplt.show()\n",
        "\n",
        "\t\tprint(\"All graphs plotted! \\n\\n\")\n",
        "\n",
        "\n",
        "def print_classification_report(pred_list, title, target_names=['Direct', 'Duplicate', 'Indirect', 'Isolated'],\n",
        "\t\t\t\t\t\t\t\tsave_result_path=\"Expt_results/results.csv\"):\n",
        "\n",
        "\tflatten = lambda l: np.array([item for sublist in l for item in sublist])\n",
        "#\tpred_list = flatten(pred_list)\n",
        "\tprint(pred_list)\n",
        "\ty_pred = np.array([x[0].tolist() for x in pred_list])\n",
        "\ty_true = np.array([x[1].tolist() for x in pred_list])\n",
        "\ty_pred = flatten(y_pred)\n",
        "\ty_true = flatten(y_true)\n",
        "\tprint(y_true)\n",
        "\tstr_title = \"Printing Classification Report : \" + title + \" \\n\\n\"\n",
        "\tprint(str_title)\n",
        "\treport = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
        "\tdf = pd.DataFrame(report)\n",
        "\tdf.to_csv(save_result_path)\n",
        "\tprint(report)\n",
        "\n",
        "\tstr_title = \"\\n\\n Printing Multilabel Confusion Matrix : \" + title + \" \\n\\n\"\n",
        "\tprint(str_title)\n",
        "\tprint(multilabel_confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\tstr_title = \"Printing Micro averaged scores : \" + title + \" \\n\\n\"\n",
        "\tprint(str_title)\n",
        "\tprint(precision_recall_fscore_support(y_true, y_pred, average='micro'))\n",
        "\tprint(\"\\n\")\n",
        "\n",
        "\tprint(\"\\n All Results Printed !! \\n\")\n",
        "\n",
        "\n",
        "def nearest_word(OOV_word, word_embedding, word2matrix):\n",
        "\ttry:\n",
        "\t\tOOV_word_embedding = word_embedding[OOV_word]\n",
        "\texcept KeyError:\n",
        "\t\tOOV_word_embedding = np.random.normal(scale=0.6, size=(1, 300))\n",
        "\tsim_max = 0\n",
        "\tfor i in range(len(word2matrix)):\n",
        "\t\tembedding = word2matrix[i]\n",
        "\t\tsim = cosine_similarity(embedding, OOV_word_embedding)\n",
        "\t\tif sim > sim_max:\n",
        "\t\t\tsim_max = sim\n",
        "\t\t\tindex_of_closest_word = i\n",
        "\n",
        "\treturn index_of_closest_word\n",
        "\n",
        "\n",
        "def save_object(obj, filename):\n",
        "\twith open(filename, 'wb') as output:  # Overwrites any existing file.\n",
        "\t\tpickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b76O33BXeozt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------MODEL ARCHITECTURE-----------------------------:\n",
        "\n",
        "class EarlyStopping:\n",
        "\t\"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "\tdef __init__(self, patience=5, verbose=False, delta=0, path_to_cpt='checkpoint.pt'):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tpatience (int): How long to wait after last time validation loss improved.\n",
        "\t\t\t\t\t\t\tDefault: 7\n",
        "\t\t\tverbose (bool): If True, prints a message for each validation loss improvement.\n",
        "\t\t\t\t\t\t\tDefault: False\n",
        "\t\t\tdelta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "\t\t\t\t\t\t\tDefault: 0\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.patience = patience\n",
        "\t\tself.verbose = verbose\n",
        "\t\tself.counter = 0\n",
        "\t\tself.best_score = None\n",
        "\t\tself.early_stop = False\n",
        "\t\tself.val_loss_min = np.Inf\n",
        "\t\tself.delta = delta\n",
        "\t\tself.path = path_to_cpt\n",
        "\n",
        "\tdef __call__(self, val_loss, model):\n",
        "\n",
        "\t\tscore = -val_loss\n",
        "\n",
        "\t\tif self.best_score is None:\n",
        "\t\t\tself.best_score = score\n",
        "\t\t\tself.save_checkpoint(val_loss, model)\n",
        "\t\telif score < self.best_score + self.delta:\n",
        "\t\t\tself.counter += 1\n",
        "\t\t\t# print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "\t\t\tif self.counter >= self.patience:\n",
        "\t\t\t\tself.early_stop = True\n",
        "\t\telse:\n",
        "\t\t\tself.best_score = score\n",
        "\t\t\tself.save_checkpoint(val_loss, model)\n",
        "\t\t\tself.counter = 0\n",
        "\n",
        "\tdef save_checkpoint(self, val_loss, model):\n",
        "\t\t'''Saves model when validation loss decrease.'''\n",
        "\t\t# if self.verbose:\n",
        "\t\t# \tprint(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "\t\ttorch.save(model.state_dict(), self.path)\n",
        "\t\tself.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "\n",
        "\tdef __init__(self, config, vocab_size, word_embeddings):\n",
        "\t\tsuper(BiLSTM, self).__init__()\n",
        "\n",
        "\t\tself.config = config\n",
        "\t\tself.loss = config.loss_fn\n",
        "\t\t# self.optimizer = optimizer\n",
        "\t\tself.dropout = self.config.dropout_keep\n",
        "\n",
        "\t\t# Layer 1: Word2Vec Embedding.\n",
        "\t\tself.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
        "\t\tself.embeddings.weight = nn.Parameter(torch.as_tensor(word_embeddings, dtype=torch.float32), requires_grad=False)\n",
        "\n",
        "\t\t# Layer 2: Bidirectional LSTM\n",
        "\t\tself.lstm = nn.LSTM(input_size=self.config.embed_size,\n",
        "\t\t\t\t\t\t\thidden_size=self.config.hidden_size,\n",
        "\t\t\t\t\t\t\tnum_layers=self.config.hidden_layers,\n",
        "\t\t\t\t\t\t\tdropout=self.config.dropout_keep,\n",
        "\t\t\t\t\t\t\tbidirectional=True,\n",
        "\t\t\t\t\t\t\tbatch_first=True)\n",
        "\n",
        "\t\t#Layer 3: Attention\n",
        "\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
        "\t\tself.W_s1 = nn.Linear(2*config.hidden_size, 350)\n",
        "\t\tself.W_s2 = nn.Linear(350, 30)\n",
        "\t\t\n",
        "\t\t# Layer 4: Rest of the layers\n",
        "\t\tself.net = nn.Sequential(nn.Linear(3, self.config.ll_hidden_size), nn.ReLU(), nn.Dropout(p=self.dropout),\n",
        "\t\t\t\t\t\t\t\t nn.Linear(self.config.ll_hidden_size, self.config.output_size), nn.Softmax())\n",
        "\n",
        "\t# net.apply(init_weights)\n",
        "\n",
        "\t#   def init_weights(self,m):\n",
        "\t#     if type(m) == nn.Linear:\n",
        "\t#         torch.nn.init.xavier_uniform(m.weight)\n",
        "\t#         m.bias.data.fill_(0.01)\n",
        "\n",
        "\t# https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/selfAttention.py\n",
        "\tdef self_attention_net(self, lstm_output):\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
        "\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
        "\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
        "\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
        "\t\tpos & neg.\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
        "\t\t---------\n",
        "\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
        "\t\t\t\t  attention to different parts of the input sentence.\n",
        "\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
        "\t\t\"\"\"\n",
        "\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
        "\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "\n",
        "\t\treturn attn_weight_matrix\n",
        "\n",
        "\t# https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n",
        "\tdef attention_net(self, lstm_output, final_state):\n",
        "\t\t\"\"\" \n",
        "\t\tNow we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
        "\t\tbetween each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
        "\t\t\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\t\t---------\t\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\t\t\t\t  \n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, seq_len)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, seq_len)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\thidden = final_state\n",
        "\t\tattn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\t\tsoft_attn_weights = F.softmax(attn_weights, 1)\n",
        "\t\tnew_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\t\t\n",
        "\t\treturn new_hidden_state\n",
        "\n",
        "\t# https://discuss.pytorch.org/t/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers/15398/2\n",
        "\t# https://stackoverflow.com/questions/49466894/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers-in-pytorch\n",
        "\tdef forward(self, pairs, batch_size):\n",
        "\t\tq1 = torch.stack([x[0] for x in pairs]).to(device)\n",
        "\t\tq2 = torch.stack([x[1] for x in pairs]).to(device)\n",
        "\n",
        "\t\t# Input: batch_size x seq_length\n",
        "\t\t# Output: batch-size x seq_length x embedding_dimension\n",
        "\t\tx1 = self.embeddings(q1)\n",
        "\t\tx2 = self.embeddings(q2)\n",
        "\n",
        "\t\t# Input: (batch_size, seq_length, input_size) (input_size=embedding_dimension in this case)\n",
        "\t\t# Output: (batch_size, seq_length, 2*hidden_size) (batch_size is dim0 since batch_first is true)\n",
        "\t\t# last_hidden_state: (2 * batch_size, hidden_size)  (2 due to bidirectional, otherwise would be 1)\n",
        "\t\t# last_cell_state: (2 * batch_size, hidden_size)    (2 due to bidirectional, otherwise would be 1)\n",
        "\t\tlstm_out1, (h_n1, c_n1) = self.lstm(x1.to(device))\n",
        "\t\tlstm_out2, (h_n2, c_n2) = self.lstm(x2.to(device))\n",
        "\n",
        "\t\t#Self Attention\n",
        "\t\tattn_weight_matrix1 = self.self_attention_net(lstm_out1)\n",
        "\t\tattn_weight_matrix2 = self.self_attention_net(lstm_out2)\n",
        "\t\t# attn_weight_matrix: (batch_size, r, seq_len)\n",
        "\t\thidden_matrix1 = torch.bmm(attn_weight_matrix1, lstm_out1)\n",
        "\t\thidden_matrix2 = torch.bmm(attn_weight_matrix2, lstm_out2)\n",
        "\t\t# hidden_matrix: (batch_size, r, 2*hidden_size)\n",
        "\n",
        "\t\t# print(\"Shape of hidden state is {} before concat\".format(h_n1.shape))\n",
        "\n",
        "\t\t# Concating both iterations of bilstm\n",
        "\t\th_n1 = torch.cat([h_n1[0, :, :], h_n1[1, :, :]], -1).view(batch_size, 2 * self.config.hidden_size)\n",
        "\t\th_n2 = torch.cat([h_n2[0, :, :], h_n2[1, :, :]], -1).view(batch_size, 2 * self.config.hidden_size)\n",
        "\n",
        "\t\t# Attention\n",
        "\t\tattn_h_n1 = self.attention_net(lstm_out1, h_n1)\n",
        "\t\tattn_h_n2 = self.attention_net(lstm_out1, h_n2)\n",
        "\n",
        "\t\t# print(\"Shape of hidden state is {} after concat and reshape\".format(h_n1.shape))\n",
        "\t\t\n",
        "\t\t# shape of hidden state = batch_size,2*hidden_size -> dot product across second dimension\n",
        "\t\tdotproduct = torch.sum(torch.mul(attn_h_n1, attn_h_n2), 1).view(batch_size, -1)\n",
        "\t\t# Shape of h_n1 => batch_size,2*hidden_size\n",
        "\n",
        "\t\treturn dotproduct\n",
        "\n",
        "\tdef calling(self, t, b, a, batch_size):\n",
        "\t\tinner_dot_titles = self.forward(t, batch_size)\n",
        "\t\tinner_dot_body = self.forward(b, batch_size)\n",
        "\t\tinner_dot_ans = self.forward(a, batch_size)\n",
        "\n",
        "\t\t# need to concatenate these tensors along the right dimention - batch size\n",
        "\t\tconcat_input_to_dense = torch.cat((inner_dot_titles, inner_dot_body, inner_dot_ans), 1)\n",
        "\n",
        "\t\t# concat_input_to_dense = concat_input_to_dense.view(-1,)\n",
        "\t\toutput = self.net(concat_input_to_dense)\n",
        "\t\treturn output.view(-1, self.config.output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBUsZ8K5eozw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------PREPROCESSING <DATA LOADERS>-------------------------------------------\n",
        "\n",
        "# Defining the Vocab class to be able to map words to indices and indices to words\n",
        "class Vocab:\n",
        "\tdef __init__(self, name):\n",
        "\t\tself.name = name\n",
        "\t\tself.word2index = {}\n",
        "\t\tself.word2count = {}\n",
        "\t\tself.index2word = {0: \"<PAD>\"}\n",
        "\t\tself.n_words = 1\n",
        "\n",
        "\tdef addWord(self, word):\n",
        "\t\tif word not in self.word2index:\n",
        "\t\t\tself.word2index[word] = self.n_words\n",
        "\t\t\tself.word2count[word] = 1\n",
        "\t\t\tself.index2word[self.n_words] = word\n",
        "\t\t\tself.n_words += 1\n",
        "\t\telse:\n",
        "\t\t\tself.word2count[word] += 1\n",
        "\n",
        "\tdef addSentence(self, sentence):\n",
        "\t\tfor word in sentence.split(' '):\n",
        "\t\t\tself.addWord(word.lower())\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "\n",
        "\tdef __init__(self, path, target_col, cols_to_include=['id', 'q1_Title', 'q1_Body', 'q1_AcceptedAnswerBody',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'q1_AnswersBody', 'q2_Title', 'q2_Body',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'q2_AcceptedAnswerBody',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'q2_AnswersBody', 'class']):\n",
        "\n",
        "\t\tself.df = pd.read_csv(path, usecols=cols_to_include)\n",
        "\t\tf = open('Data/Cleaners.pickle', 'rb')\n",
        "\t\tloaded_obj = pickle.load(f)\n",
        "\t\tf.close()\n",
        "\t\tself.target_col = target_col\n",
        "\t\tself.punctuation_remove = loaded_obj['punctuations']\n",
        "\t\tself.misspelt_words = loaded_obj['mispell_dict']\n",
        "\t\tself.cols_to_include = cols_to_include\n",
        "\t\tself.new_rest_cols = [\"id\", \"q1_Title\", \"q2_Title\", \"q1_Body\", \"q2_Body\", \"answer_text1\", \"answer_text2\"]\n",
        "\n",
        "\tdef removing_stop_words_lemmatize(self, text, stop_words=[]):\n",
        "\t\tfiltered_text = []\n",
        "\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\tsent_list = [sent for sent in sent_tokenize(text)]\n",
        "\t\tfiltered_sentence = []\n",
        "\t\tfor sent in sent_list:\n",
        "\t\t\tfiltered_sentence.append(\n",
        "\t\t\t\t' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(sent) if word not in stop_words]))\n",
        "\t\t\t# print(filtered_sentence)\n",
        "\t\tfiltered_text.append('.'.join(filtered_sentence))\n",
        "\t\treturn filtered_text\n",
        "\n",
        "\tdef cleaning_text(self, text):\n",
        "\t\tfor punct in self.punctuation_remove:\n",
        "\t\t\ttext = text.replace(punct, '')\n",
        "\n",
        "\t\tfor wrong, norm in self.misspelt_words.items():\n",
        "\t\t\ttobereplaced = '[' + wrong + ']' + '+'\n",
        "\t\t\treplace = norm\n",
        "\t\t\tre.sub(tobereplaced, replace, text.lower())\n",
        "\n",
        "\t\ttext = re.sub(r\"\\s\\s+\", \" \", text)\n",
        "\t\ttext = text.strip()\n",
        "\n",
        "\t\treturn text\n",
        "\n",
        "\tdef encoding_target(self, df):\n",
        "\t\tprint(\"-> Encoding target.../n\")\n",
        "\t\tlabel_enc = LabelEncoder()\n",
        "\t\tdf[self.target_col] = label_enc.fit_transform(df[self.target_col])\n",
        "\t\treturn df\n",
        "\n",
        "\tdef replace_encoding(self, num):\n",
        "\t\tif num == 0: return np.array([1, 0, 0, 0]).astype('int64')\n",
        "\t\tif num == 1: return np.array([0, 1, 0, 0]).astype('int64')\n",
        "\t\tif num == 2: return np.array([0, 0, 1, 0]).astype('int64')\n",
        "\t\tif num == 3: return np.array([0, 0, 0, 1]).astype('int64')\n",
        "\n",
        "\tdef run(self, stop_words=[]):\n",
        "\t\tprint(\"Starting Preprocessing.../n\")\n",
        "\t\tstop_words = stop_words + list(set(stopwords.words('english')))\n",
        "\t\tfor cols in self.cols_to_include:\n",
        "\t\t\tif cols not in [self.target_col, 'id']:\n",
        "\t\t\t\tfiltered_text = []\n",
        "\t\t\t\tfor i in range(len(self.df)):\n",
        "\t\t\t\t\ttext = self.df.iloc[i][cols]\n",
        "\t\t\t\t\ttext = self.cleaning_text(text)\n",
        "\t\t\t\t\ttext = self.removing_stop_words_lemmatize(text, stop_words)\n",
        "\t\t\t\t\tfiltered_text.append(text[0])\n",
        "\t\t\t\tself.df[cols] = filtered_text\n",
        "\t\t\t\tprint(\"-->Done for - \", cols)\n",
        "\n",
        "\t\tself.df = self.encoding_target(self.df)\n",
        "\t\tself.df[self.target_col] = self.df[self.target_col].apply(lambda x: self.replace_encoding(int(x)))\n",
        "\n",
        "\t\tself.df['answer_text1'] = self.df['q1_AcceptedAnswerBody'] + self.df['q1_AnswersBody']\n",
        "\t\tself.df['answer_text2'] = self.df['q2_AcceptedAnswerBody'] + self.df['q2_AnswersBody']\n",
        "\t\tself.df = self.df.drop(['q1_AcceptedAnswerBody', 'q2_AcceptedAnswerBody', 'q1_AnswersBody', 'q2_AnswersBody'],\n",
        "\t\t\t\t\t\t\t   axis=1)\n",
        "\n",
        "\t\tself.df = self.df[[\"id\", \"q1_Title\", \"q2_Title\", \"q1_Body\", \"q2_Body\", \"answer_text1\", \"answer_text2\", \"class\"]]\n",
        "\n",
        "\t\tprint(\"Done!/n\")\n",
        "\t\treturn self.df, self.new_rest_cols\n",
        "\n",
        "\n",
        "class Bilstm_Dataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, df, col, target_col):\n",
        "\t\t# rest_col = [col for col in rest_col if col not in ['id']]\n",
        "\t\tself.feats = torch.as_tensor(list(df[col].values.tolist()), dtype=torch.long, device=device)\n",
        "\t\tself.target = torch.as_tensor(list(df[target_col].values.tolist()), dtype=torch.long, device=device)\n",
        "\t\tself.nsamples = len(df)\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.nsamples\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\treturn self.feats[index], self.target[index]\n",
        "\n",
        "\n",
        "class forming_batches:\n",
        "\tdef __init__(self, vocab, trimsize_to_col_dict, df, target, vocab_new=True):\n",
        "\t\tself.mapping_trimsize = trimsize_to_col_dict\n",
        "\t\tself.df = df\n",
        "\t\tself.target = target\n",
        "\t\tself.vocab = vocab\n",
        "\n",
        "\t\tif vocab_new:\n",
        "\t\t\tfor column in list(self.df.columns):\n",
        "\t\t\t\tif column == self.target or column == 'id': continue\n",
        "\t\t\t\tfor item in self.df[column]:\n",
        "\t\t\t\t\tfor i in item.split('.'):\n",
        "\t\t\t\t\t\tself.vocab.addSentence(i.lower())\n",
        "\t\t\tprint(\"Vocabulary formed ! \")\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Using existing Vocab object! \")\n",
        "\n",
        "\tdef extracting_indices(self, vocab, sentence):\n",
        "\t\treturn [vocab.word2index[word] for word in sentence.split(' ')]  # need to remove '.' also\n",
        "\n",
        "\tdef trim(self, mapping_trimsize, data):\n",
        "\n",
        "\t\tfor colname, maxlen in mapping_trimsize.items():\n",
        "\t\t\ttrimmed_res = []\n",
        "\t\t\tfor i in range(len(data)):\n",
        "\t\t\t\tif len(data[colname].iloc[i]) > maxlen:\n",
        "\t\t\t\t\ttrimmed_res.append(data[colname].iloc[i][0:maxlen])\n",
        "\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tdiff = maxlen - len(data[colname].iloc[i])\n",
        "\t\t\t\t\ttrimmed_res.append(data[colname].iloc[i] + [0] * diff)\n",
        "\t\t\tdata[colname] = trimmed_res\n",
        "\n",
        "\t\treturn data\n",
        "\n",
        "\tdef run(self):\n",
        "\t\tprint(\"Converting words to indices using inverse dictionary...\")\n",
        "\t\tfor col in self.df.columns:\n",
        "\t\t\tif col in [self.target, 'id']: continue\n",
        "\t\t\tresult_word2index = []\n",
        "\t\t\tfor i in range(len(self.df)):\n",
        "\t\t\t\tresult = []\n",
        "\t\t\t\tfor sent in self.df[col].iloc[i].split('.'):\n",
        "\t\t\t\t\tresult = result + self.extracting_indices(self.vocab, sent.lower())\n",
        "\n",
        "\t\t\t\tresult_word2index.append(result)\n",
        "\t\t\tself.df[col] = result_word2index\n",
        "\t\tprint(\"Trimming columns...\")\n",
        "\t\tself.df = self.trim(self.mapping_trimsize, self.df)\n",
        "\n",
        "\t\tfor col in self.df.columns:\n",
        "\t\t\tself.df[col] = self.df[col].apply(lambda x: np.array(x).astype('int64').squeeze())\n",
        "\n",
        "\t\tprint(\"dtype \", type(self.df['q1_Title'].iloc[2]))\n",
        "\n",
        "\t\treturn self.df, self.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWDoACJaeoz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------TRAIN---------------------------------------------:\n",
        "def embeddings_gen(vocab, path_to_glove):\n",
        "\tmatrix_len = len(vocab.word2index)\n",
        "\tweights_matrix = np.zeros((matrix_len + 1, 300))\n",
        "\twords_found = 0\n",
        "\n",
        "\t#     infile = open(path_to_glove,'rb')\n",
        "\t#     glove = pickle.load(infile)\n",
        "\t#     infile.close()\n",
        "\tglove = KeyedVectors.load_word2vec_format(path_to_glove, binary=False)\n",
        "\n",
        "\tfor index in vocab.index2word:\n",
        "\t\tif index == 0:\n",
        "\t\t\tweights_matrix[index] = np.zeros((1, 300))\n",
        "\t\ttry:\n",
        "\t\t\tweights_matrix[index] = glove[vocab.index2word[index]]\n",
        "\t\t\twords_found = words_found + 1\n",
        "\t\texcept KeyError:\n",
        "\t\t\tweights_matrix[index] = np.random.normal(scale=0.6, size=(1, 300))\n",
        "\n",
        "\tembedding_matrix = torch.FloatTensor(weights_matrix)\n",
        "\tprint(\"Words found in embedding:\" + str(words_found))\n",
        "\n",
        "\tdel glove\n",
        "\n",
        "\treturn embedding_matrix\n",
        "\n",
        "\n",
        "def data_loading(train_path, val_path, preprocess, target, config,\n",
        "\t\t\t\t rest_col=['id', 'q1_Title', 'q1_Body', 'q1_AcceptedAnswerBody',\n",
        "\t\t\t\t\t\t   'q1_AnswersBody', 'q2_Title', 'q2_Body', 'q2_AcceptedAnswerBody',\n",
        "\t\t\t\t\t\t   'q2_AnswersBody'],\n",
        "\t\t\t\t mapping_trimsize={'q1_Title': 10, 'q1_Body': 60, 'answer_text1': 180, 'q2_Title': 10, 'q2_Body': 60,\n",
        "\t\t\t\t\t\t\t\t   'answer_text2': 180}):\n",
        "\tif val_path is None:\n",
        "\t\tif preprocess:\n",
        "\t\t\tprint(rest_col.append(target))\n",
        "\t\t\tpreprocess_class = Preprocessing(train_path, target)\n",
        "\t\t\tdf, new_cols = preprocess_class.run()\n",
        "\t\t\tprint(\" Writing preprocessed data for future use..\")\n",
        "\t\t\tdf.to_csv(train_path[:-4] + \"_preprocessed.csv\")\n",
        "\t\telse:\n",
        "\t\t\trest_col=[\"id\", \"q1_Title\", \"q2_Title\", \"q1_Body\", \"q2_Body\", \"answer_text1\", \"answer_text2\", \"class\"]\n",
        "\t\t\tdf = pd.read_csv(train_path, usecols=rest_col)\n",
        "\n",
        "\t\tvocab = Vocab('stack')\n",
        "\n",
        "\t\tbatchify_obj = forming_batches(vocab, mapping_trimsize, df, target)\n",
        "\n",
        "\t\tdf, vocab = batchify_obj.run()\n",
        "\t\tprint(df.head())\n",
        "\n",
        "\t\tprint(\"\\n\\n Sequence of columns : \")\n",
        "\t\trest_col = [col for col in list(df.columns) if col not in ['id']]\n",
        "\t\tprint(rest_col[0:2].append(rest_col[-1]))\n",
        "\n",
        "\t\tdataset_title = Bilstm_Dataset(df, rest_col[0:2], rest_col[-1])\n",
        "\t\tdataset_body = Bilstm_Dataset(df, rest_col[2:4], rest_col[-1])\n",
        "\t\tdataset_answer = Bilstm_Dataset(df, rest_col[4:6], rest_col[-1])\n",
        "\n",
        "\t\tNUM_INSTANCES = dataset_title.__len__()\n",
        "\t\tNUM_INSTANCES = NUM_INSTANCES * config.sample\n",
        "\t\tTEST_SIZE = int(NUM_INSTANCES * config.split_ratio)\n",
        "\n",
        "\t\tnum_batches_train = (NUM_INSTANCES - TEST_SIZE) / config.batch_size\n",
        "\t\tnum_batches_val = TEST_SIZE / config.batch_size\n",
        "\n",
        "\t\tindices = list(range(NUM_INSTANCES))\n",
        "\n",
        "\t\tval_idx = np.random.choice(indices, size=TEST_SIZE, replace=False)\n",
        "\t\ttrain_idx = list(set(indices) - set(val_idx))\n",
        "\t\ttrain_sampler, val_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(val_idx)\n",
        "\n",
        "\t\ttrain_loader_title = DataLoader(dataset_title, batch_size=config.batch_size, sampler=train_sampler)\n",
        "\t\tval_loader_title = DataLoader(dataset_title, batch_size=config.batch_size, sampler=val_sampler)\n",
        "\n",
        "\t\ttrain_loader_body = DataLoader(dataset_body, batch_size=config.batch_size, sampler=train_sampler)\n",
        "\t\tval_loader_body = DataLoader(dataset_body, batch_size=config.batch_size, sampler=val_sampler)\n",
        "\n",
        "\t\ttrain_loader_ans = DataLoader(dataset_answer, batch_size=config.batch_size, sampler=train_sampler)\n",
        "\t\tval_loader_ans = DataLoader(dataset_answer, batch_size=config.batch_size, sampler=val_sampler)\n",
        "\n",
        "\t\ttrain_loaders = [train_loader_title, train_loader_body, train_loader_ans]\n",
        "\t\tval_loaders = [val_loader_title, val_loader_body, val_loader_ans]\n",
        "\n",
        "\t\tdel train_loader_title, train_loader_body, train_loader_ans, val_loader_title, val_loader_body, val_loader_ans\n",
        "\t\tdel dataset_title, dataset_body, dataset_answer\n",
        "\n",
        "\t\treturn train_loaders, val_loaders, vocab, int(num_batches_train), int(num_batches_val)\n",
        "\telse:\n",
        "\t\tif preprocess:\n",
        "\t\t\tprint(rest_col.append(target))\n",
        "\t\t\tpreprocess_class = Preprocessing(train_path, target)\n",
        "\t\t\tdf, new_cols = preprocess_class.run()\n",
        "\n",
        "\t\t\tpreprocess_class_val = Preprocessing(val_path, target)\n",
        "\t\t\tdf_val, new_cols = preprocess_class_val.run()\n",
        "\n",
        "\t\t\tprint(\" Writing preprocessed data for future use..\")\n",
        "\t\t\tdf.to_csv(train_path[:-4] + \"_preprocessed.csv\")\n",
        "\t\t\tdf_val.to_csv(val_path[:-4] + \"_preprocessed.csv\")\n",
        "\n",
        "\t\telse:\n",
        "\t\t\trest_col=[\"id\", \"q1_Title\", \"q2_Title\", \"q1_Body\", \"q2_Body\", \"answer_text1\", \"answer_text2\", \"class\"]\t\t\t\n",
        "\t\t\tdf = pd.read_csv(train_path, usecols=rest_col)\n",
        "\t\t\tdf_val = pd.read_csv(val_path, usecols=rest_col)\n",
        "\n",
        "\t\tvocab = Vocab('stack')\n",
        "\n",
        "\t\tbatchify_obj = forming_batches(vocab, mapping_trimsize, df, target, vocab_new=True)\n",
        "\t\tdf, vocab = batchify_obj.run()\n",
        "\n",
        "\t\tbatchify_obj_val = forming_batches(vocab, mapping_trimsize, df_val, target, vocab_new=True)\n",
        "\t\tdf_val, vocab = batchify_obj_val.run()\n",
        "\n",
        "\t\tprint(df.head())\n",
        "\n",
        "\t\tprint(\"\\n\\n Sequence of columns : \")\n",
        "\t\trest_col = [col for col in list(df.columns) if col not in ['id']]\n",
        "\t\tprint(rest_col)\n",
        "\n",
        "\t\tdataset_title = Bilstm_Dataset(df, rest_col[0:2], rest_col[-1])\n",
        "\t\tdataset_body = Bilstm_Dataset(df, rest_col[2:4], rest_col[-1])\n",
        "\t\tdataset_answer = Bilstm_Dataset(df, rest_col[4:6], rest_col[-1])\n",
        "\n",
        "\t\tdataset_title_val = Bilstm_Dataset(df_val, rest_col[0:2], rest_col[-1])\n",
        "\t\tdataset_body_val = Bilstm_Dataset(df_val, rest_col[2:4], rest_col[-1])\n",
        "\t\tdataset_answer_val = Bilstm_Dataset(df_val, rest_col[4:6], rest_col[-1])\n",
        "\n",
        "\t\tNUM_INSTANCES_TRAIN = dataset_title.__len__()\n",
        "\t\tNUM_INSTANCES_TRAIN = NUM_INSTANCES_TRAIN * config.sample\n",
        "\t\tNUM_INSTANCES_VAL = dataset_title_val.__len__()\n",
        "\t\tNUM_INSTANCES_VAL = NUM_INSTANCES_VAL * config.sample\n",
        "\n",
        "\t\tnum_batches_train = NUM_INSTANCES_TRAIN / config.batch_size\n",
        "\t\tnum_batches_val = NUM_INSTANCES_VAL / config.batch_size\n",
        "\n",
        "\t\tindices_train = list(range(NUM_INSTANCES_TRAIN))\n",
        "\t\tindices_val = list(range(NUM_INSTANCES_VAL))\n",
        "\n",
        "\t\tval_idx = np.random.choice(indices_val, size=NUM_INSTANCES_VAL, replace=False)\n",
        "\t\ttrain_idx = np.random.choice(indices_train, size=NUM_INSTANCES_TRAIN, replace=False)\n",
        "\t\ttrain_sampler, val_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(val_idx)\n",
        "\n",
        "\t\ttrain_loader_title = DataLoader(dataset_title, batch_size=config.batch_size, sampler=train_sampler)\n",
        "\t\tval_loader_title = DataLoader(dataset_title_val, batch_size=config.batch_size, sampler=val_sampler)\n",
        "\n",
        "\t\ttrain_loader_body = DataLoader(dataset_body, batch_size=config.batch_size, sampler=train_sampler)\n",
        "\t\tval_loader_body = DataLoader(dataset_body_val, batch_size=config.batch_size, sampler=val_sampler)\n",
        "\n",
        "\t\ttrain_loader_ans = DataLoader(dataset_answer, batch_size=config.batch_size, sampler=train_sampler)\n",
        "\t\tval_loader_ans = DataLoader(dataset_answer_val, batch_size=config.batch_size, sampler=val_sampler)\n",
        "\n",
        "\t\ttrain_loaders = [train_loader_title, train_loader_body, train_loader_ans]\n",
        "\t\tval_loaders = [val_loader_title, val_loader_body, val_loader_ans]\n",
        "\n",
        "\t\tdel train_loader_title, train_loader_body, train_loader_ans, val_loader_title, val_loader_body, val_loader_ans\n",
        "\t\tdel dataset_title, dataset_body, dataset_answer, dataset_title_val, dataset_body_val, dataset_answer_val\n",
        "\n",
        "\t\treturn train_loaders, val_loaders, vocab, int(num_batches_train), int(num_batches_val)\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader, num_batches, batch_size):\n",
        "\ttitle_iter = iter(loader[0])\n",
        "\tbody_iter = iter(loader[1])\n",
        "\tans_iter = iter(loader[2])\n",
        "\n",
        "\trunning_loss = 0\n",
        "\trunning_corrects = 0\n",
        "\tpred_true = []\n",
        "\tfor i in range(num_batches):\n",
        "\t\ttitle = next(title_iter)  # ith batch\n",
        "\t\tbody = next(body_iter)  # ith batch\n",
        "\t\tans = next(ans_iter)  # ith batch\n",
        "\n",
        "\t\ty_pred = model.calling(title[0].to(device), body[0].to(device), ans[0].to(device), batch_size)\n",
        "\t\tlabels = title[1]\n",
        "\n",
        "\t\t# print(\"Shape of y pred2 {}\".format(y_pred.shape))\n",
        "\t\t# print(\"Shape of y true2 {}\".format(labels.shape))\n",
        "\n",
        "\t\tpred_true.append([torch.argmax(y_pred, 1), torch.argmax(labels, 1)])\n",
        "\n",
        "\t\tloss = model.loss(y_pred, torch.argmax(labels, 1))\n",
        "\t\trunning_loss = running_loss + loss.item()\n",
        "\t\trunning_corrects = running_corrects + torch.sum(torch.argmax(y_pred, 1) == torch.argmax(labels, 1)).item()\n",
        "\n",
        "\tepoch_loss = 1.0 * running_loss / num_batches\n",
        "\tepoch_acc = 1.0 * running_corrects / num_batches\n",
        "\n",
        "\treturn epoch_loss, epoch_acc, pred_true\n",
        "\n",
        "\n",
        "def run_train(model, train_loader, val_loader, epoch, num_batches_train, num_batches_val, batch_size, optimizer):\n",
        "\ttrain_losses = []\n",
        "\tval_accuracies = []\n",
        "\tval_losses = []\n",
        "\tf1 = []\n",
        "\tlosses = []\n",
        "\n",
        "\ttitle_iter = iter(train_loader[0])\n",
        "\tbody_iter = iter(train_loader[1])\n",
        "\tans_iter = iter(train_loader[2])\n",
        "\n",
        "\tmodel.train()\n",
        "\tfor i in tqdm.trange(num_batches_train, file=sys.stdout, desc='Iterations'):\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\ttitle = next(title_iter)  # ith batch\n",
        "\t\tbody = next(body_iter)  # ith batch\n",
        "\t\tans = next(ans_iter)  # ith batch\n",
        "\n",
        "\t\t# print(\"Shape of train title iter {}\".format(title[0].shape))\n",
        "\n",
        "\t\ty_pred = model.calling(title[0].to(device), body[0].to(device), ans[0].to(device), batch_size)\n",
        "\t\ty_true = title[1]\n",
        "\n",
        "\t\t# print(\"Shape of y pred {}\".format(y_pred.shape))\n",
        "\t\t# print(\"Shape of y true {}\".format(y_true.shape))\n",
        "\n",
        "\t\tloss = model.loss(y_pred, torch.argmax(y_true, 1))\n",
        "\t\tloss.backward()\n",
        "\t\tlosses.append(loss.detach().cpu().numpy())\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tif i % 300 == 0:\n",
        "\t\t\tprint(\"Iter: {},Epoch: {}\\n\".format(i + 1, epoch))\n",
        "\t\t\tavg_train_loss = np.mean(losses)\n",
        "\t\t\ttrain_losses.append(avg_train_loss)\n",
        "\t\t\tprint(\"\\tAverage training loss: {:.5f}\\n\".format(avg_train_loss))\n",
        "\t\t\tlosses = []\n",
        "\t\t\tmodel.eval()\n",
        "\t\t\t# Evalute Accuracy on validation set\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tval_loss, val_accuracy, curr_F1 = evaluate_model(model, val_loader, num_batches_val, batch_size)\n",
        "\t\t\t\tval_accuracies.append(val_accuracy)\n",
        "\t\t\t\tval_losses.append(val_loss)\n",
        "\t\t\t\tf1.append(curr_F1)\n",
        "\n",
        "\t\t\t\tprint(\"\\tVal Accuracy: {:.4f}\\n\".format(val_accuracy))\n",
        "\t\t\t\tprint(\"\\n\")\n",
        "\t\t\t\tmodel.train()\n",
        "\n",
        "\treturn train_losses, val_losses, val_accuracies, f1\n",
        "\n",
        "\n",
        "def train_model(path_to_data, path_vocab_save, path_embed_matrix_save, train_file, val_file, path_to_glove,\n",
        "\t\t\t\tpath_to_cpt, config, preprocess=False):\n",
        "\ttrain_path = path_to_data + '/' + train_file\n",
        "\tif val_file is None:\n",
        "\t\tval_path = None\n",
        "\telse:\n",
        "\t\tval_path = path_to_data + '/' + val_file\n",
        "\n",
        "\ttrain_loaders, val_loaders, vocab, num_batches_train, num_batches_val = data_loading(train_path, val_path,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t preprocess, target='class',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t config=config)\n",
        "\n",
        "\tbest_val_loss = float(\"inf\")\n",
        "\n",
        "\tembedding_matrix = embeddings_gen(vocab, path_to_glove)\n",
        "\n",
        "\t# Saving vocab object and embedding matrix\n",
        "\tsave_object(vocab, path_vocab_save)\n",
        "\tsave_object(embedding_matrix, path_embed_matrix_save)\n",
        "\n",
        "\tmodel = BiLSTM(config, len(vocab.word2index), embedding_matrix)\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "\t# initialize the early_stopping object\n",
        "\tearly_stopping = EarlyStopping(patience=config.patience, verbose=True, delta=config.delta, path_to_cpt=path_to_cpt)\n",
        "\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tmodel.to(device)\n",
        "\n",
        "\tmodel.train()\n",
        "\n",
        "\ttrain_losses_plot = []\n",
        "\tval_accuracies_plot = []\n",
        "\tval_losses_plot = []\n",
        "\tepoch_f1 = []\n",
        "\n",
        "\tstart_of_training = time.time()\n",
        "\n",
        "\tfor i in range(config.epochs):\n",
        "\t\tprint(\"Epoch: {}\".format(i))\n",
        "\t\ttrain_loss, val_loss, val_accuracy, all_F1 = run_train(model, train_loaders, val_loaders, i, num_batches_train,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   num_batches_val, config.batch_size, optimizer)\n",
        "\n",
        "\t\ttrain_losses_plot.append(train_loss)\n",
        "\t\tval_losses_plot.append(val_loss)\n",
        "\t\tval_accuracies_plot.append(val_accuracy)\n",
        "\t\tepoch_f1.append(all_F1)\n",
        "\n",
        "\t\tearly_stopping(val_loss[-1], model)\n",
        "\n",
        "\t\tif early_stopping.early_stop:\n",
        "\t\t\tprint(\"Early stopping....\\n\")\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tavg_train_losses = np.mean(np.array(train_loss))\n",
        "\t\tavg_val_losses = np.mean(np.array(val_loss))\n",
        "\t\tif avg_val_losses < best_val_loss:\n",
        "\t\t\tbest_val_loss = avg_val_losses\n",
        "\t\t\tbest_model = model\n",
        "\n",
        "\tend_of_training = time.time()\n",
        "\n",
        "\tprint(\"\\n\\n Training Time : {:5.2f} secs\".format(end_of_training - start_of_training))\n",
        "\n",
        "\t# load the last checkpoint with the best model\n",
        "\tmodel.load_state_dict(torch.load(path_to_cpt))\n",
        "\n",
        "\treturn model, avg_train_losses, avg_val_losses, train_losses_plot, val_accuracies_plot, val_losses_plot, epoch_f1, vocab, embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoayJRujeoz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------The MAIN CODE TO RUN---------------------------------------:\n",
        "\n",
        "#Arguments for path and other boolean values: <Can be changed for running on colab/kaggle>\n",
        "path_to_data = Data\n",
        "path_vocab_save = Expt_results/Vocab.pkl\n",
        "path_embed_matrix = Expt_results/EmbedMatrix.pkl\n",
        "path_to_cpt = Expt_results/checkpoints/checkpoint.pt\n",
        "path_to_glove = Data/glove.840B.300d.word2vec.txt\n",
        "model_path = \n",
        "save_result_path = Expt_results/results.csv\n",
        "name_train = train_sample.csv\n",
        "name_val = None\n",
        "name_test = train_sample.csv\n",
        "target_names = ['Direct', 'Duplicate', 'Indirect', 'Isolated']\n",
        "figname = ['Training.png', 'Validation.png']\n",
        "mode = train_&_test\n",
        "to_preprocess = True\n",
        "smooth = False\n",
        "\n",
        "#setting seeds for reproducibility\n",
        "torch.manual_seed(100)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(100)\n",
        "np.random.seed(100)\n",
        "\n",
        "#-------------------------------------------CUDA <might be a need to define above>---------------------------:\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "#\tdevice=\"cpu\"\n",
        "config = Config()\n",
        "\n",
        "test_path = path_to_data + '/' + name_test\n",
        "\n",
        "if mode == \"train_&_test\":\n",
        "    model, avg_train_losses, avg_val_losses, train_losses_plot, val_accuracies_plot, val_losses_plot, epoch_f1, vocab, embedding_matrix = train_model(\n",
        "    path_to_data, path_vocab_save, path_embed_matrix, name_train, name_val,\n",
        "    path_to_glove, path_to_cpt, config, to_preprocess)\n",
        "    plot = plot_results(train_losses_plot, val_losses_plot, val_accuracies_plot, figname, smooth)\n",
        "#\t\tplot.run(figure_sep=True)\n",
        "elif mode == \"only_test\":\n",
        "  # unpickling vocab and embed_metrix\n",
        "  infile = open(path_vocab_save, 'rb')\n",
        "  vocab = pickle.load(infile)\n",
        "  infile.close()\n",
        "\n",
        "  infile = open(args.path_embed_matrix, 'rb')\n",
        "  embedding_matrix = pickle.load(infile)\n",
        "  infile.close()\n",
        "\n",
        "  # Unloading the best model saved in last session\n",
        "  model = BiLSTM(config, len(vocab.word2index), embedding_matrix).to(device)\n",
        "  model.load_state_dict(torch.load(args.model_path))\n",
        "  model.eval()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "test_loss, test_acc, test_pred_true = run_test(test_path, model, vocab, embedding_matrix, config,\n",
        "                         args.to_preprocess, target='class')\n",
        "\n",
        "# Only for Test rn - we can modify later\n",
        "print_classification_report(test_pred_true, args.title, args.target_names, args.save_result_path)\n",
        "\n",
        "#-----------------------------------------------------END------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}